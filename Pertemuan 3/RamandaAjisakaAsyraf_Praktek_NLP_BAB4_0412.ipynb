{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled0.ipynb","provenance":[],"collapsed_sections":["VCDJNbwlcf-R","bqK0hqdZc1q_","-0SsoWDzdfYI","Pmk8YgtXs-nG","HcaBaw0eu6jN","vKTfHpI9xRJx","NXZdyUMFyp0p"],"authorship_tag":"ABX9TyNzoby9MvtTJ7y/WLXRj4G4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Transforming Text into Data Structures\n","\n","Nama  : Ramanda Ajisaka Asyraf\n","\n","NPM   : 20312067\n","\n","Kelas : IF Gab 1"],"metadata":{"id":"b3YUNbX6bew6"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YEDAcDr9OfGe","executionInfo":{"status":"ok","timestamp":1649774600836,"user_tz":-420,"elapsed":35289,"user":{"displayName":"RamandaAA Mahasiswa","userId":"07619045368833697713"}},"outputId":"4dffce74-0092-47e3-fa5e-a85db3c983d9"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["## Matrix Representation\n","\n","kita dapat membangun sebuah vektor dan matriks berdasarkan data text dengan menggunakan modul **CountVectorizer** dari **sklearn**"],"metadata":{"id":"VCDJNbwlcf-R"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uxe0DP3DZvDW","executionInfo":{"status":"ok","timestamp":1649727938680,"user_tz":-420,"elapsed":424,"user":{"displayName":"RamandaAA Mahasiswa","userId":"07619045368833697713"}},"outputId":"0190a345-f60a-4dcf-e6da-d517d22b3ea7"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'computers': 2, 'analyze': 1, 'text': 7, 'using': 8, 'vectors': 9, 'matrices': 5, 'process': 6, 'massive': 4, 'amounts': 0, 'data': 3}\n","[[0 1 1 0 0 0 0 1 0 0]\n"," [0 0 0 0 0 1 0 0 1 1]\n"," [1 0 1 1 1 0 1 1 0 0]]\n"]}],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","X = (\"Computers can analyze text\",\n","     \"They do it using vectors and matrices\",\n","     \"Computers can process massive amounts of text data\")\n","\n","vectorizer = CountVectorizer(stop_words='english')\n","X_vec = vectorizer.fit_transform(X)\n","print(vectorizer.vocabulary_)\n","print(X_vec.todense()) "]},{"cell_type":"markdown","source":["## CountVectorizer for Bag of Words Model\n","kita dapat membangun vocabolary dengan metode BoW"],"metadata":{"id":"bqK0hqdZc1q_"}},{"cell_type":"code","source":["import nltk\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","from nltk.corpus import stopwords\n","from nltk.stem.porter import PorterStemmer \n","from nltk.stem.snowball import SnowballStemmer\n","from nltk.stem.wordnet import WordNetLemmatizer\n","import pandas as pd\n","import re\n","import numpy as np\n","from sklearn.feature_extraction.text import CountVectorizer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oqVy4lv9dIqB","executionInfo":{"status":"ok","timestamp":1649728143300,"user_tz":-420,"elapsed":3060,"user":{"displayName":"RamandaAA Mahasiswa","userId":"07619045368833697713"}},"outputId":"b80cd07e-4257-4957-adc4-0a625bb308cf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"]}]},{"cell_type":"markdown","source":["### Building a corpus of sentences"],"metadata":{"id":"-0SsoWDzdfYI"}},{"cell_type":"code","source":["sentences = [\"We are reading about Natural Language Processing Here\",\n","            \"Natural Language Processing making computers comprehend language data\",\n","            \"The field of Natural Language Processing is evolving everyday\"]"],"metadata":{"id":"ld7DRCC0dhcf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["corpus = pd.Series(sentences)\n","corpus"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2jEMIS9wdosZ","executionInfo":{"status":"ok","timestamp":1649728199708,"user_tz":-420,"elapsed":458,"user":{"displayName":"RamandaAA Mahasiswa","userId":"07619045368833697713"}},"outputId":"3446d987-50db-4911-f6f3-91633a02e4a5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    We are reading about Natural Language Processi...\n","1    Natural Language Processing making computers c...\n","2    The field of Natural Language Processing is ev...\n","dtype: object"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["#### Data preprocessing pipeline"],"metadata":{"id":"yxL0prtJd0lX"}},{"cell_type":"code","source":["def text_clean(corpus, keep_list):\n","    '''\n","    Purpose : Function to keep only alphabets, digits and certain words (punctuations, qmarks, tabs etc. removed)\n","    \n","    Input : Takes a text corpus, 'corpus' to be cleaned along with a list of words, 'keep_list', which have to be retained\n","            even after the cleaning process\n","    \n","    Output : Returns the cleaned text corpus\n","    \n","    '''\n","    cleaned_corpus = pd.Series()\n","    for row in corpus:\n","        qs = []\n","        for word in row.split():\n","            if word not in keep_list:\n","                p1 = re.sub(pattern='[^a-zA-Z0-9]',repl=' ',string=word)\n","                p1 = p1.lower()\n","                qs.append(p1)\n","            else : qs.append(word)\n","        cleaned_corpus = cleaned_corpus.append(pd.Series(' '.join(qs)))\n","    return cleaned_corpus"],"metadata":{"id":"DmcQOrpNdytP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def lemmatize(corpus):\n","    lem = WordNetLemmatizer()\n","    corpus = [[lem.lemmatize(x, pos = 'v') for x in x] for x in corpus]\n","    return corpus"],"metadata":{"id":"Uq0BJ6B-d-Rv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def stem(corpus, stem_type = None):\n","    if stem_type == 'snowball':\n","        stemmer = SnowballStemmer(language = 'english')\n","        corpus = [[stemmer.stem(x) for x in x] for x in corpus]\n","    else :\n","        stemmer = PorterStemmer()\n","        corpus = [[stemmer.stem(x) for x in x] for x in corpus]\n","    return corpus"],"metadata":{"id":"vnnRSNXMeIig"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def stopwords_removal(corpus):\n","    wh_words = ['who', 'what', 'when', 'why', 'how', 'which', 'where', 'whom']\n","    stop = set(stopwords.words('english'))\n","    for word in wh_words:\n","        stop.remove(word)\n","    corpus = [[x for x in x.split() if x not in stop] for x in corpus]\n","    return corpus"],"metadata":{"id":"PXQx91vXeLrv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def preprocess(corpus, keep_list, cleaning = True, stemming = False, stem_type = None, lemmatization = False, remove_stopwords = True):\n","    '''\n","    Purpose : Function to perform all pre-processing tasks (cleaning, stemming, lemmatization, stopwords removal etc.)\n","    \n","    Input : \n","    'corpus' - Text corpus on which pre-processing tasks will be performed\n","    'keep_list' - List of words to be retained during cleaning process\n","    'cleaning', 'stemming', 'lemmatization', 'remove_stopwords' - Boolean variables indicating whether a particular task should \n","                                                                  be performed or not\n","    'stem_type' - Choose between Porter stemmer or Snowball(Porter2) stemmer. Default is \"None\", which corresponds to Porter\n","                  Stemmer. 'snowball' corresponds to Snowball Stemmer\n","    \n","    Note : Either stemming or lemmatization should be used. There's no benefit of using both of them together\n","    \n","    Output : Returns the processed text corpus\n","    \n","    '''\n","    \n","    if cleaning == True:\n","        corpus = text_clean(corpus, keep_list)\n","    \n","    if remove_stopwords == True:\n","        corpus = stopwords_removal(corpus)\n","    else :\n","        corpus = [[x for x in x.split()] for x in corpus]\n","    \n","    if lemmatization == True:\n","        corpus = lemmatize(corpus)\n","        \n","        \n","    if stemming == True:\n","        corpus = stem(corpus, stem_type)\n","    \n","    corpus = [' '.join(x) for x in corpus]        \n","\n","    return corpus"],"metadata":{"id":"h7zANyVLeZK_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Preprocessing with Lemmatization here\n","preprocessed_corpus = preprocess(corpus, keep_list = [], stemming = False, stem_type = None,\n","                                lemmatization = True, remove_stopwords = True)\n","preprocessed_corpus"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uudaHm7lecLg","executionInfo":{"status":"ok","timestamp":1649728422996,"user_tz":-420,"elapsed":2392,"user":{"displayName":"RamandaAA Mahasiswa","userId":"07619045368833697713"}},"outputId":"5417117f-75ea-4295-db32-7d6c61836aa1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n","  # This is added back by InteractiveShellApp.init_path()\n"]},{"output_type":"execute_result","data":{"text/plain":["['read natural language process',\n"," 'natural language process make computers comprehend language data',\n"," 'field natural language process evolve everyday']"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["### CountVectorizer\n","merupakan sebuah API yang mempermudah kita untuk membangun BoW model dengan cara mengkonversi dokumen teks kedalam matriks"],"metadata":{"id":"SPIPvvG1fNtm"}},{"cell_type":"code","source":["vectorizer = CountVectorizer()\n","bow_matrix = vectorizer.fit_transform(preprocessed_corpus)"],"metadata":{"id":"ahnZFJB1fRc3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(vectorizer.get_feature_names())\n","print(bow_matrix.toarray())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aSXjd0lZr_BD","executionInfo":{"status":"ok","timestamp":1649731958881,"user_tz":-420,"elapsed":389,"user":{"displayName":"RamandaAA Mahasiswa","userId":"07619045368833697713"}},"outputId":"be3aca14-7489-42d4-86af-77051dedd995"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['comprehend', 'computers', 'data', 'everyday', 'evolve', 'field', 'language', 'make', 'natural', 'process', 'read']\n","[[0 0 0 0 0 0 1 0 1 1 1]\n"," [1 1 1 0 0 0 2 1 1 1 0]\n"," [0 0 0 1 1 1 1 0 1 1 0]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n","  warnings.warn(msg, category=FutureWarning)\n"]}]},{"cell_type":"code","source":["print(bow_matrix.toarray().shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"et6TeSYXsCgq","executionInfo":{"status":"ok","timestamp":1649731974891,"user_tz":-420,"elapsed":1068,"user":{"displayName":"RamandaAA Mahasiswa","userId":"07619045368833697713"}},"outputId":"c45cd62f-21b6-41b5-a2e7-cbff8cb7d57c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(3, 11)\n"]}]},{"cell_type":"markdown","source":["### Features offered by CountVectorizer"],"metadata":{"id":"UBL2Wt27sWHE"}},{"cell_type":"code","source":["vectorizer_ngram_range = CountVectorizer(analyzer='word', ngram_range=(1,3))\n","bow_matrix_ngram = vectorizer_ngram_range.fit_transform(preprocessed_corpus)"],"metadata":{"id":"RmZHzm4ssag7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(vectorizer_ngram_range.get_feature_names())\n","print(bow_matrix_ngram.toarray())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sCacuauWshcS","executionInfo":{"status":"ok","timestamp":1649732109195,"user_tz":-420,"elapsed":520,"user":{"displayName":"RamandaAA Mahasiswa","userId":"07619045368833697713"}},"outputId":"76c58eba-2dc5-4557-9be8-49e80752e7b3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['comprehend', 'comprehend language', 'comprehend language data', 'computers', 'computers comprehend', 'computers comprehend language', 'data', 'everyday', 'evolve', 'evolve everyday', 'field', 'field natural', 'field natural language', 'language', 'language data', 'language process', 'language process evolve', 'language process make', 'make', 'make computers', 'make computers comprehend', 'natural', 'natural language', 'natural language process', 'process', 'process evolve', 'process evolve everyday', 'process make', 'process make computers', 'read', 'read natural', 'read natural language']\n","[[0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1]\n"," [1 1 1 1 1 1 1 0 0 0 0 0 0 2 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0]\n"," [0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n","  warnings.warn(msg, category=FutureWarning)\n"]}]},{"cell_type":"markdown","source":["#### Understanding Max Features\n","max_features yang akan membangun kosakata sedemikian rupa sehingga ukuran kosakata\n","akan kurang dari atau sama dengan max_features yang dipesan oleh frekuensi token yang terjadi\n","dalam korpus, seperti yang diilustrasikan dalam blok kode berikut"],"metadata":{"id":"Pmk8YgtXs-nG"}},{"cell_type":"code","source":["vectorizer_max_features = CountVectorizer(analyzer='word', ngram_range=(1,3), max_features = 6)\n","bow_matrix_max_features = vectorizer_max_features.fit_transform(preprocessed_corpus)"],"metadata":{"id":"VP8CLqiStvQS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(vectorizer_max_features.get_feature_names())\n","print(bow_matrix_max_features.toarray())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sfKwOFjyt9tK","executionInfo":{"status":"ok","timestamp":1649732478422,"user_tz":-420,"elapsed":611,"user":{"displayName":"RamandaAA Mahasiswa","userId":"07619045368833697713"}},"outputId":"025134d5-8e70-4d25-8d1a-6d5fe8768ba5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['language', 'language process', 'natural', 'natural language', 'natural language process', 'process']\n","[[1 1 1 1 1 1]\n"," [2 1 1 1 1 1]\n"," [1 1 1 1 1 1]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n","  warnings.warn(msg, category=FutureWarning)\n"]}]},{"cell_type":"markdown","source":["#### Thresholding using Max_df and Min_df\n","max_df dan min_df dapat dilakukan dan\n","akibatnya memberikan ambang batas minimum dan maksimum terhadap terjadinya\n","frasa dalam korpus"],"metadata":{"id":"hIZtSHAGuSki"}},{"cell_type":"code","source":["vectorizer_max_features = CountVectorizer(analyzer='word', ngram_range=(1,3), max_df = 3, min_df = 2)\n","bow_matrix_max_features = vectorizer_max_features.fit_transform(preprocessed_corpus)"],"metadata":{"id":"BbXMWzv-uncj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(vectorizer_max_features.get_feature_names())\n","print(bow_matrix_max_features.toarray())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S8o8hyHUuq8C","executionInfo":{"status":"ok","timestamp":1649732663370,"user_tz":-420,"elapsed":561,"user":{"displayName":"RamandaAA Mahasiswa","userId":"07619045368833697713"}},"outputId":"7b723a2f-5706-4ea4-96bc-7d2cc25282d7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['language', 'language process', 'natural', 'natural language', 'natural language process', 'process']\n","[[1 1 1 1 1 1]\n"," [2 1 1 1 1 1]\n"," [1 1 1 1 1 1]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n","  warnings.warn(msg, category=FutureWarning)\n"]}]},{"cell_type":"markdown","source":["## Term Frequency-Inverse Document Frequency based Vectorizer"],"metadata":{"id":"HcaBaw0eu6jN"}},{"cell_type":"markdown","source":["### TfIdfVectorizer\n","Vectorizer TF-IDF dasar dapat instantiated, seperti yang ditunjukkan dalam dua langkah yang ditunjukkan dalam\n","cuplikan kode berikut. Langkah kedua memungkinkan data dipasang ke TF-IDF\n","vectorizer, diikuti oleh transformasi data menjadi bentuk vektor TF-IDF menggunakan\n","fit_transform"],"metadata":{"id":"Scrk18rMwQaB"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","vectorizer = TfidfVectorizer()\n","tf_idf_matrix = vectorizer.fit_transform(preprocessed_corpus)"],"metadata":{"id":"urkOGXgGwOzK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(vectorizer.get_feature_names())\n","print(tf_idf_matrix.toarray())\n","print(\"\\nThe shape of the TF-IDF matrix is: \", tf_idf_matrix.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nmtH8cCjxDSh","executionInfo":{"status":"ok","timestamp":1649733287537,"user_tz":-420,"elapsed":460,"user":{"displayName":"RamandaAA Mahasiswa","userId":"07619045368833697713"}},"outputId":"1745e4c9-eae1-4b70-f2c0-c19ff85968cf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['comprehend', 'computers', 'data', 'everyday', 'evolve', 'field', 'language', 'make', 'natural', 'process', 'read']\n","[[0.         0.         0.         0.         0.         0.\n","  0.41285857 0.         0.41285857 0.41285857 0.69903033]\n"," [0.40512186 0.40512186 0.40512186 0.         0.         0.\n","  0.478543   0.40512186 0.2392715  0.2392715  0.        ]\n"," [0.         0.         0.         0.49711994 0.49711994 0.49711994\n","  0.29360705 0.         0.29360705 0.29360705 0.        ]]\n","\n","The shape of the TF-IDF matrix is:  (3, 11)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n","  warnings.warn(msg, category=FutureWarning)\n"]}]},{"cell_type":"markdown","source":["### Changing the norm to l1, default option is l2 which was used above\n","l2: Sum of squares of vector elements is 1.\n","\n","l1: Sum of absolute values of vector elements is 1."],"metadata":{"id":"vKTfHpI9xRJx"}},{"cell_type":"code","source":["vectorizer_l1_norm = TfidfVectorizer(norm=\"l1\")\n","tf_idf_matrix_l1_norm = vectorizer_l1_norm.fit_transform(preprocessed_corpus)"],"metadata":{"id":"uETzDJKdxZmB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(vectorizer_l1_norm.get_feature_names())\n","print(tf_idf_matrix_l1_norm.toarray())\n","print(\"\\nThe shape of the TF-IDF matrix is: \", tf_idf_matrix_l1_norm.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"41Fb0AM8xd6R","executionInfo":{"status":"ok","timestamp":1649733396009,"user_tz":-420,"elapsed":449,"user":{"displayName":"RamandaAA Mahasiswa","userId":"07619045368833697713"}},"outputId":"97d74015-1b92-407e-be6d-617bd7965ed2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['comprehend', 'computers', 'data', 'everyday', 'evolve', 'field', 'language', 'make', 'natural', 'process', 'read']\n","[[0.         0.         0.         0.         0.         0.\n","  0.21307663 0.         0.21307663 0.21307663 0.3607701 ]\n"," [0.1571718  0.1571718  0.1571718  0.         0.         0.\n","  0.1856564  0.1571718  0.0928282  0.0928282  0.        ]\n"," [0.         0.         0.         0.2095624  0.2095624  0.2095624\n","  0.12377093 0.         0.12377093 0.12377093 0.        ]]\n","\n","The shape of the TF-IDF matrix is:  (3, 11)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n","  warnings.warn(msg, category=FutureWarning)\n"]}]},{"cell_type":"markdown","source":["### N-grams and Max features with TfidfVectorizer\n","Mirip dengan CountVectorizer, vectorizer TF-IDF menawarkan kemampuan menggunakan n-gram.\n","dan max_features untuk membatasi kosa kata kita"],"metadata":{"id":"M4dw37agxyhp"}},{"cell_type":"code","source":["vectorizer_n_gram_max_features = TfidfVectorizer(norm=\"l2\", analyzer='word', ngram_range=(1,3), max_features = 6)\n","tf_idf_matrix_n_gram_max_features = vectorizer_n_gram_max_features.fit_transform(preprocessed_corpus)"],"metadata":{"id":"uOlGCnBWx8Nb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(vectorizer_n_gram_max_features.get_feature_names())\n","print(tf_idf_matrix_n_gram_max_features.toarray())\n","print(\"\\nThe shape of the TF-IDF matrix is: \", tf_idf_matrix_n_gram_max_features.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hLe8zO8tx_1K","executionInfo":{"status":"ok","timestamp":1649733535561,"user_tz":-420,"elapsed":389,"user":{"displayName":"RamandaAA Mahasiswa","userId":"07619045368833697713"}},"outputId":"585e4f4a-7793-44fc-d1f4-0172cf446d5f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['language', 'language process', 'natural', 'natural language', 'natural language process', 'process']\n","[[0.40824829 0.40824829 0.40824829 0.40824829 0.40824829 0.40824829]\n"," [0.66666667 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333]\n"," [0.40824829 0.40824829 0.40824829 0.40824829 0.40824829 0.40824829]]\n","\n","The shape of the TF-IDF matrix is:  (3, 6)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n","  warnings.warn(msg, category=FutureWarning)\n"]}]},{"cell_type":"markdown","source":["## Cosine Similarity\n","Kesamaan cosine membantu dalam mengukur cosine dari sudut antara dua vektor. Si\n","nilai kesamaan cosine akan terletak pada kisaran -1 hingga +1."],"metadata":{"id":"NXZdyUMFyp0p"}},{"cell_type":"markdown","source":["### Measuring Cosine Similarity between Document Vectors"],"metadata":{"id":"jVnfBU0Ky2zZ"}},{"cell_type":"markdown","source":["#### Cosine Similarity Calculation"],"metadata":{"id":"Nr1dvo-pzPDw"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","def cosine_similarity(vector1, vector2):\n","    vector1 = np.array(vector1)\n","    vector2 = np.array(vector2)\n","    return np.dot(vector1, vector2) / (np.sqrt(np.sum(vector1**2)) * np.sqrt(np.sum(vector2**2)))"],"metadata":{"id":"WfWDOTH-zBBE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### CountVectorizer"],"metadata":{"id":"Aj333i0czXTQ"}},{"cell_type":"code","source":["vectorizer = CountVectorizer()\n","bow_matrix = vectorizer.fit_transform(preprocessed_corpus)"],"metadata":{"id":"SrCUYEgTzhsr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(vectorizer.get_feature_names())\n","print(bow_matrix.toarray())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"84FZjavzzk1b","executionInfo":{"status":"ok","timestamp":1649733956883,"user_tz":-420,"elapsed":429,"user":{"displayName":"RamandaAA Mahasiswa","userId":"07619045368833697713"}},"outputId":"8b95c65a-ddfe-47f7-d072-8d401d5ed9dc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['comprehend', 'computers', 'data', 'everyday', 'evolve', 'field', 'language', 'make', 'natural', 'process', 'read']\n","[[0 0 0 0 0 0 1 0 1 1 1]\n"," [1 1 1 0 0 0 2 1 1 1 0]\n"," [0 0 0 1 1 1 1 0 1 1 0]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n","  warnings.warn(msg, category=FutureWarning)\n"]}]},{"cell_type":"markdown","source":["#### Cosine similarity between the document vectors built using CountVectorizer"],"metadata":{"id":"93afq7LLz1xS"}},{"cell_type":"code","source":["for i in range(bow_matrix.shape[0]):\n","    for j in range(i + 1, bow_matrix.shape[0]):\n","        print(\"The cosine similarity between the documents \", i, \"and\", j, \"is: \",\n","              cosine_similarity(bow_matrix.toarray()[i], bow_matrix.toarray()[j]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jZWukX8Ez3Zr","executionInfo":{"status":"ok","timestamp":1649734032156,"user_tz":-420,"elapsed":527,"user":{"displayName":"RamandaAA Mahasiswa","userId":"07619045368833697713"}},"outputId":"5ee18f8e-5013-45a1-d06a-aa47b5827f3f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The cosine similarity between the documents  0 and 1 is:  0.6324555320336759\n","The cosine similarity between the documents  0 and 2 is:  0.6123724356957946\n","The cosine similarity between the documents  1 and 2 is:  0.5163977794943223\n"]}]},{"cell_type":"markdown","source":["#### TfidfVectorizer"],"metadata":{"id":"OyBtZwr0z90B"}},{"cell_type":"code","source":["vectorizer = TfidfVectorizer()\n","tf_idf_matrix = vectorizer.fit_transform(preprocessed_corpus)"],"metadata":{"id":"4G4reSGYz_dY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(vectorizer.get_feature_names())\n","print(tf_idf_matrix.toarray())\n","print(\"\\nThe shape of the TF-IDF matrix is: \", tf_idf_matrix.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dS8xZTKo0CtI","executionInfo":{"status":"ok","timestamp":1649734076513,"user_tz":-420,"elapsed":500,"user":{"displayName":"RamandaAA Mahasiswa","userId":"07619045368833697713"}},"outputId":"da22004f-f370-458a-a7aa-893f54f3c210"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['comprehend', 'computers', 'data', 'everyday', 'evolve', 'field', 'language', 'make', 'natural', 'process', 'read']\n","[[0.         0.         0.         0.         0.         0.\n","  0.41285857 0.         0.41285857 0.41285857 0.69903033]\n"," [0.40512186 0.40512186 0.40512186 0.         0.         0.\n","  0.478543   0.40512186 0.2392715  0.2392715  0.        ]\n"," [0.         0.         0.         0.49711994 0.49711994 0.49711994\n","  0.29360705 0.         0.29360705 0.29360705 0.        ]]\n","\n","The shape of the TF-IDF matrix is:  (3, 11)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n","  warnings.warn(msg, category=FutureWarning)\n"]}]},{"cell_type":"markdown","source":["#### Cosine similarity between the document vectors built using TfidfVectorizer"],"metadata":{"id":"t6xVvL4r0VXi"}},{"cell_type":"code","source":["for i in range(tf_idf_matrix.shape[0]):\n","    for j in range(i + 1, tf_idf_matrix.shape[0]):\n","        print(\"The cosine similarity between the documents \", i, \"and\", j, \"is: \",\n","              cosine_similarity(tf_idf_matrix.toarray()[i], tf_idf_matrix.toarray()[j]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a6QTFVni0XPS","executionInfo":{"status":"ok","timestamp":1649734162092,"user_tz":-420,"elapsed":521,"user":{"displayName":"RamandaAA Mahasiswa","userId":"07619045368833697713"}},"outputId":"4d568a68-6099-4236-da98-90953f16eef8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The cosine similarity between the documents  0 and 1 is:  0.39514115766749125\n","The cosine similarity between the documents  0 and 2 is:  0.36365455673761865\n","The cosine similarity between the documents  1 and 2 is:  0.2810071916500233\n"]}]},{"cell_type":"markdown","source":["## One Hot Vectors"],"metadata":{"id":"ZT3oXvq_1DgR"}},{"cell_type":"code","source":["sentence = [\"We are reading about Natural Language Processing Here\"]\n","corpus = pd.Series(sentence)\n","corpus\n","# Preprocessing with Lemmatization here\n","preprocessed_corpus = preprocess(corpus, keep_list = [], stemming = False, stem_type = None,\n","                                lemmatization = True, remove_stopwords = True)\n","preprocessed_corpus"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-xk60p8j18Xh","executionInfo":{"status":"ok","timestamp":1649734644219,"user_tz":-420,"elapsed":502,"user":{"displayName":"RamandaAA Mahasiswa","userId":"07619045368833697713"}},"outputId":"a5780590-c132-49d9-f4f0-f88951ae34d1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n","  # This is added back by InteractiveShellApp.init_path()\n"]},{"output_type":"execute_result","data":{"text/plain":["['read natural language process']"]},"metadata":{},"execution_count":38}]},{"cell_type":"markdown","source":["### Building the vocabulary"],"metadata":{"id":"2Cd2nEbn2VVC"}},{"cell_type":"code","source":["set_of_words = set()\n","for word in preprocessed_corpus[0].split():\n","    set_of_words.add(word)\n","vocab = list(set_of_words)\n","print(vocab)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RS3tkSYP2WmI","executionInfo":{"status":"ok","timestamp":1649734685255,"user_tz":-420,"elapsed":462,"user":{"displayName":"RamandaAA Mahasiswa","userId":"07619045368833697713"}},"outputId":"3816435d-554c-4ee3-b8d5-48560ed5b7ed"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['process', 'natural', 'read', 'language']\n"]}]},{"cell_type":"markdown","source":["### Fetching the position of each word in the vocabulary"],"metadata":{"id":"vsR5kj0g2a45"}},{"cell_type":"code","source":["position = {}\n","for i, token in enumerate(vocab):\n","    position[token] = i\n","print(position)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ARiJ-hi92cMg","executionInfo":{"status":"ok","timestamp":1649734704070,"user_tz":-420,"elapsed":410,"user":{"displayName":"RamandaAA Mahasiswa","userId":"07619045368833697713"}},"outputId":"4b985ebf-ca51-4324-fd21-a6f7f6751a19"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'process': 0, 'natural': 1, 'read': 2, 'language': 3}\n"]}]},{"cell_type":"markdown","source":["### Instantiating the one hot matrix"],"metadata":{"id":"VQwvCJNC2eZA"}},{"cell_type":"code","source":["one_hot_matrix = np.zeros((len(preprocessed_corpus[0].split()), len(vocab)))\n","one_hot_matrix.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hvDyenTV2hgA","executionInfo":{"status":"ok","timestamp":1649734727258,"user_tz":-420,"elapsed":440,"user":{"displayName":"RamandaAA Mahasiswa","userId":"07619045368833697713"}},"outputId":"220a039e-b9fa-4de6-b04e-af30e43f2eec"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(4, 4)"]},"metadata":{},"execution_count":41}]},{"cell_type":"markdown","source":["### Building One Hot Vectors"],"metadata":{"id":"_CTcXms42kd5"}},{"cell_type":"code","source":["for i, token in enumerate(preprocessed_corpus[0].split()):\n","    one_hot_matrix[i][position[token]] = 1"],"metadata":{"id":"NklQNRdB2kFg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Visualizing the One Hot Vectors"],"metadata":{"id":"K11or-SB27GA"}},{"cell_type":"code","source":["one_hot_matrix"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S19tWYYL3Dsg","executionInfo":{"status":"ok","timestamp":1649734862195,"user_tz":-420,"elapsed":517,"user":{"displayName":"RamandaAA Mahasiswa","userId":"07619045368833697713"}},"outputId":"7609b89c-4080-4b02-d294-18f919588ebf"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0., 0., 1., 0.],\n","       [0., 1., 0., 0.],\n","       [0., 0., 0., 1.],\n","       [1., 0., 0., 0.]])"]},"metadata":{},"execution_count":43}]},{"cell_type":"markdown","source":["## Building a basic chatbot"],"metadata":{"id":"gVEhTfYY3PJw"}},{"cell_type":"code","source":["#!pip install scikit-learn\n","import numpy as np\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","\n","#loading questions and answers in separate lists\n","import ast \n","questions = []\n","answers = [] \n","with open('/content/drive/MyDrive/Colab Notebooks/PBA/qa_Electronics.json','r') as f:\n","    for line in f:\n","        data = ast.literal_eval(line)\n","        questions.append(data['question'].lower())\n","        answers.append(data['answer'].lower())"],"metadata":{"id":"UQOG0MDU3ZGo","executionInfo":{"status":"ok","timestamp":1649774710664,"user_tz":-420,"elapsed":13494,"user":{"displayName":"RamandaAA Mahasiswa","userId":"07619045368833697713"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# tokenize the text and convert data in matrix format\n","from sklearn.feature_extraction.text import CountVectorizer\n","vectorizer = CountVectorizer(stop_words='english')\n","X_vec = vectorizer.fit_transform(questions)"],"metadata":{"id":"5SDm7iyWPIM_","executionInfo":{"status":"ok","timestamp":1649774736141,"user_tz":-420,"elapsed":4117,"user":{"displayName":"RamandaAA Mahasiswa","userId":"07619045368833697713"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Transform data by applying term frequency inverse document frequency (TF-IDF) \n","tfidf = TfidfTransformer() #by default applies \"l2\" normalization\n","X_tfidf = tfidf.fit_transform(X_vec)"],"metadata":{"id":"nGIPC1tkPOKU","executionInfo":{"status":"ok","timestamp":1649774751316,"user_tz":-420,"elapsed":587,"user":{"displayName":"RamandaAA Mahasiswa","userId":"07619045368833697713"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def conversation(im):\n","    global tfidf, answers, X_tfidf\n","    Y_vec = vectorizer.transform(im)\n","    Y_tfidf = tfidf.fit_transform(Y_vec)\n","    cos_sim = np.rad2deg(np.arccos(max(cosine_similarity(Y_tfidf, X_tfidf)[0])))\n","    if cos_sim > 60 :\n","        return \"sorry, I did not quite understand that\"\n","    else:\n","        return answers[np.argmax(cosine_similarity(Y_tfidf, X_tfidf)[0])]\n","\n","def main():\n","    usr = input(\"Please enter your username: \")\n","    print(\"support: Hi, welcome to Q&A support. How can I help you?\")\n","    while True:\n","        im = input(\"{}: \".format(usr))\n","        if im.lower() == 'bye':\n","            print(\"Q&A support: bye!\")\n","            break\n","        else:\n","            print(\"Q&A support: \"+conversation([im]))"],"metadata":{"id":"cFsFeM89PXad","executionInfo":{"status":"ok","timestamp":1649774788417,"user_tz":-420,"elapsed":465,"user":{"displayName":"RamandaAA Mahasiswa","userId":"07619045368833697713"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DAMqbJ07PaZs","executionInfo":{"status":"ok","timestamp":1649774896613,"user_tz":-420,"elapsed":24592,"user":{"displayName":"RamandaAA Mahasiswa","userId":"07619045368833697713"}},"outputId":"e54f2490-d514-457c-ce32-2b3c35bfb07e"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Please enter your username: ramanda\n","support: Hi, welcome to Q&A support. How can I help you?\n","ramanda: bluescreen\n","Q&A support: goes completely black\n","ramanda: how to repair\n","Q&A support: i have never had any problem that needed any type of repair. pioneer is a long time trusted name in sound electronics. there are hundreds of pioneer repair centers around the country. i had no worries about that. it can be difficult to operate sometimes mostly because it uses wifi. sometimes i've had to turn it off and then back on to get it working again. performance has been great.\n","ramanda: bye\n","Q&A support: bye!\n"]}]},{"cell_type":"markdown","source":["## Bag of Words in Action"],"metadata":{"id":"Guji37qxQREe"}},{"cell_type":"code","source":["import nltk\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","from nltk.corpus import stopwords\n","from nltk.stem.porter import PorterStemmer \n","from nltk.stem.snowball import SnowballStemmer\n","from nltk.stem.wordnet import WordNetLemmatizer\n","import pandas as pd\n","import re\n","import numpy as np\n","\n","# Take in a list of sentences\n","sentences = [\"We are reading about Natural Language Processing Here\",\n","            \"Natural Language Processing making computers comprehend language data\",\n","            \"The field of Natural Language Processing is evolving everyday\"]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wXveWeEVQbDs","executionInfo":{"status":"ok","timestamp":1649775144052,"user_tz":-420,"elapsed":443,"user":{"displayName":"RamandaAA Mahasiswa","userId":"07619045368833697713"}},"outputId":"05cf50b6-c482-47da-89e0-76ad02e5c242"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}]},{"cell_type":"markdown","source":["### Create a Pandas Series of the object\n"],"metadata":{"id":"UUzp9oLpQwM1"}},{"cell_type":"code","source":["corpus = pd.Series(sentences)\n","corpus"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iflwHVSQQ4Zs","executionInfo":{"status":"ok","timestamp":1649775217485,"user_tz":-420,"elapsed":7,"user":{"displayName":"RamandaAA Mahasiswa","userId":"07619045368833697713"}},"outputId":"27afa8dc-6c1e-4621-fd62-48564ee7f246"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    We are reading about Natural Language Processi...\n","1    Natural Language Processing making computers c...\n","2    The field of Natural Language Processing is ev...\n","dtype: object"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["### Data preprocessing"],"metadata":{"id":"31DGrWgdQ7Mc"}},{"cell_type":"code","source":["def text_clean(corpus, keep_list):\n","    '''\n","    Purpose : Function to keep only alphabets, digits and certain words (punctuations, qmarks, tabs etc. removed)\n","    \n","    Input : Takes a text corpus, 'corpus' to be cleaned along with a list of words, 'keep_list', which have to be retained\n","            even after the cleaning process\n","    \n","    Output : Returns the cleaned text corpus\n","    \n","    '''\n","    cleaned_corpus = pd.Series()\n","    for row in corpus:\n","        qs = []\n","        for word in row.split():\n","            if word not in keep_list:\n","                p1 = re.sub(pattern='[^a-zA-Z0-9]',repl=' ',string=word)\n","                p1 = p1.lower()\n","                qs.append(p1)\n","            else : qs.append(word)\n","        cleaned_corpus = cleaned_corpus.append(pd.Series(' '.join(qs)))\n","    return cleaned_corpus"],"metadata":{"id":"dCRWqCjtQ8XV","executionInfo":{"status":"ok","timestamp":1649775221788,"user_tz":-420,"elapsed":381,"user":{"displayName":"RamandaAA Mahasiswa","userId":"07619045368833697713"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["def stopwords_removal(corpus):\n","    wh_words = ['who', 'what', 'when', 'why', 'how', 'which', 'where', 'whom']\n","    stop = set(stopwords.words('english'))\n","    for word in wh_words:\n","        stop.remove(word)\n","    corpus = [[x for x in x.split() if x not in stop] for x in corpus]\n","    return corpus"],"metadata":{"id":"u9BD7RDbRFa0","executionInfo":{"status":"ok","timestamp":1649775239022,"user_tz":-420,"elapsed":533,"user":{"displayName":"RamandaAA Mahasiswa","userId":"07619045368833697713"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["def lemmatize(corpus):\n","    lem = WordNetLemmatizer()\n","    corpus = [[lem.lemmatize(x, pos = 'v') for x in x] for x in corpus]\n","    return corpus"],"metadata":{"id":"VCPVvJ9qRIAF","executionInfo":{"status":"ok","timestamp":1649775249627,"user_tz":-420,"elapsed":378,"user":{"displayName":"RamandaAA Mahasiswa","userId":"07619045368833697713"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["def stem(corpus, stem_type = None):\n","    if stem_type == 'snowball':\n","        stemmer = SnowballStemmer(language = 'english')\n","        corpus = [[stemmer.stem(x) for x in x] for x in corpus]\n","    else :\n","        stemmer = PorterStemmer()\n","        corpus = [[stemmer.stem(x) for x in x] for x in corpus]\n","    return corpus"],"metadata":{"id":"UO1FTYHaRKj1","executionInfo":{"status":"ok","timestamp":1649775260248,"user_tz":-420,"elapsed":453,"user":{"displayName":"RamandaAA Mahasiswa","userId":"07619045368833697713"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["def preprocess(corpus, keep_list, cleaning = True, stemming = False, stem_type = None, lemmatization = False, remove_stopwords = True):\n","    '''\n","    Purpose : Function to perform all pre-processing tasks (cleaning, stemming, lemmatization, stopwords removal etc.)\n","    \n","    Input : \n","    'corpus' - Text corpus on which pre-processing tasks will be performed\n","    'keep_list' - List of words to be retained during cleaning process\n","    'cleaning', 'stemming', 'lemmatization', 'remove_stopwords' - Boolean variables indicating whether a particular task should \n","                                                                  be performed or not\n","    'stem_type' - Choose between Porter stemmer or Snowball(Porter2) stemmer. Default is \"None\", which corresponds to Porter\n","                  Stemmer. 'snowball' corresponds to Snowball Stemmer\n","    \n","    Note : Either stemming or lemmatization should be used. There's no benefit of using both of them together\n","    \n","    Output : Returns the processed text corpus\n","    \n","    '''\n","    \n","    if cleaning == True:\n","        corpus = text_clean(corpus, keep_list)\n","    \n","    if remove_stopwords == True:\n","        corpus = stopwords_removal(corpus)\n","    else :\n","        corpus = [[x for x in x.split()] for x in corpus]\n","    \n","    if lemmatization == True:\n","        corpus = lemmatize(corpus)\n","        \n","        \n","    if stemming == True:\n","        corpus = stem(corpus, stem_type)\n","    \n","    corpus = [' '.join(x) for x in corpus]        \n","\n","    return corpus"],"metadata":{"id":"KYeMmmcNRNW1","executionInfo":{"status":"ok","timestamp":1649775272310,"user_tz":-420,"elapsed":396,"user":{"displayName":"RamandaAA Mahasiswa","userId":"07619045368833697713"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["common_dot_words = ['U.S.', 'Mr.', 'Mrs.', 'D.C.']"],"metadata":{"id":"AZEQ_Tb7RRod","executionInfo":{"status":"ok","timestamp":1649775292337,"user_tz":-420,"elapsed":13,"user":{"displayName":"RamandaAA Mahasiswa","userId":"07619045368833697713"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["# Preprocessing with Lemmatization here\n","preprocessed_corpus = preprocess(corpus, keep_list = common_dot_words, stemming = False, stem_type = None,\n","                                lemmatization = True, remove_stopwords = True)\n","preprocessed_corpus"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B5q-ZZMyRVJ3","executionInfo":{"status":"ok","timestamp":1649775305531,"user_tz":-420,"elapsed":2616,"user":{"displayName":"RamandaAA Mahasiswa","userId":"07619045368833697713"}},"outputId":"04972c1b-a19f-4dc6-d595-02f3612d9e65"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n","  # This is added back by InteractiveShellApp.init_path()\n"]},{"output_type":"execute_result","data":{"text/plain":["['read natural language process',\n"," 'natural language process make computers comprehend language data',\n"," 'field natural language process evolve everyday']"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","source":["### Building the vocabulary"],"metadata":{"id":"6df_txboRXUs"}},{"cell_type":"code","source":["set_of_words = set()\n","for sentence in preprocessed_corpus:\n","    for word in sentence.split():\n","        set_of_words.add(word)\n","vocab = list(set_of_words)\n","print(vocab)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"04jyF0euRZhL","executionInfo":{"status":"ok","timestamp":1649775324745,"user_tz":-420,"elapsed":375,"user":{"displayName":"RamandaAA Mahasiswa","userId":"07619045368833697713"}},"outputId":"145db153-a94c-443c-a60f-99a2a596f5eb"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["['everyday', 'evolve', 'field', 'language', 'read', 'computers', 'data', 'make', 'process', 'comprehend', 'natural']\n"]}]},{"cell_type":"markdown","source":["### Fetching the position of each word in the vocabulary"],"metadata":{"id":"PkJccqRJRcMj"}},{"cell_type":"code","source":["position = {}\n","for i, token in enumerate(vocab):\n","    position[token] = i\n","print(position)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BnPRYMUeRdU2","executionInfo":{"status":"ok","timestamp":1649775342153,"user_tz":-420,"elapsed":5,"user":{"displayName":"RamandaAA Mahasiswa","userId":"07619045368833697713"}},"outputId":"d0bc0130-1580-4ea5-ca9a-cadbdda25a77"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["{'everyday': 0, 'evolve': 1, 'field': 2, 'language': 3, 'read': 4, 'computers': 5, 'data': 6, 'make': 7, 'process': 8, 'comprehend': 9, 'natural': 10}\n"]}]},{"cell_type":"markdown","source":["### Creating a matrix to hold the Bag of Words representation"],"metadata":{"id":"Fzcyd6vfRhjj"}},{"cell_type":"code","source":["bow_matrix = np.zeros((len(preprocessed_corpus), len(vocab)))"],"metadata":{"id":"FpKgxzrNRi20","executionInfo":{"status":"ok","timestamp":1649775364058,"user_tz":-420,"elapsed":5,"user":{"displayName":"RamandaAA Mahasiswa","userId":"07619045368833697713"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["for i, preprocessed_sentence in enumerate(preprocessed_corpus):\n","    for token in preprocessed_sentence.split():   \n","        bow_matrix[i][position[token]] = bow_matrix[i][position[token]] + 1"],"metadata":{"id":"qnKkBpaLRkzk","executionInfo":{"status":"ok","timestamp":1649775371929,"user_tz":-420,"elapsed":442,"user":{"displayName":"RamandaAA Mahasiswa","userId":"07619045368833697713"}}},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":["### Bag of Words representation"],"metadata":{"id":"7AeSHGMgR13e"}},{"cell_type":"code","source":["bow_matrix"],"metadata":{"id":"d1bDmhqKR4n9","executionInfo":{"status":"ok","timestamp":1649775449210,"user_tz":-420,"elapsed":431,"user":{"displayName":"RamandaAA Mahasiswa","userId":"07619045368833697713"}},"outputId":"90745985-6ab0-4ee8-bf36-d909f440f34c","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1.],\n","       [0., 0., 0., 2., 0., 1., 1., 1., 1., 1., 1.],\n","       [1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1.]])"]},"metadata":{},"execution_count":23}]}]}